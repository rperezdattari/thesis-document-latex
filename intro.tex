\begin{intro}
\section{Motivation and Problem Statement}
In  recent years, outstanding results in complex decision-making problems have been obtained with Deep Reinforcement Learning (DRL). State of the art algorithms have solved problems with large state spaces and discrete action spaces, such as playing Atari games \cite{atari}, or beating the world champion in GO \cite{Silver2016}, along with low level simulated continuous control tasks in environments such as the ones included in the OpenAI Gym \cite{brockman2016openai} and the DeepMind Control Suite \cite{tassa2018deepmind}. Learning policies, parameterized with Convolutional Neural Networks (CNN) for high-dimensional state spaces, such as raw images, gives agents the possibility to build rich state representations of the environment without feature engineering on the side of the designer (which was always necessary in classical RL). These properties can be really useful in complex robotics problems, giving robots the ability to solve problems using raw visual information. 

Nevertheless, DRL has several limitations when used to address real world systems \cite{Gu2017}. For instance, DRL algorithms require large amounts of data, which means long training times that in contrast to simulated environments cannot be accelerated with more computational power. If somehow this shortcoming was addressed, sometimes the reward function would still pose a problem as it is hard to specify/model and/or to compute in many cases in the real-world. For instance, sometimes additional perception capabilities to the ones of the agent are needed for computing the reward function, since in theory the reward is given ``by the environment``, not be the agent.

In this regard, the transfer of knowledge learned in a simulator to the real-world is a typical solution. However the mismatch between the virtual and real environment, known as ``Reality Gap``, is often problematic \cite{koos2013transferability}. This results in agents that do not perform at their best in the real-world. Thus, it would be preferable to learn/fine-tune policies directly in the real-world.

On the other hand, machine learning methods that rely on the transfer of human knowledge to an agent have shown to be time efficient for obtaining good performance policies. Moreover, some methods do not need expert human teachers for training high performance agents \cite{akrour2011preference,Knox:2009:ISA:1597735.1597738,Celemin2018AnInteractive}. This is why they appear to be good candidates to tackle the DRL real-world issues mentioned before. 

Therefore, in this work, we study the use of human corrective feedback during task execution, to learn policies with low and high dimensional state spaces, in continuous action problems using deep neural networks. Our work extends D-COACH \cite{perez2018interactive}, which is a Deep Learning (DL) based extension of the COrrective Advice Communicated by Humans (COACH) framework \cite{Celemin2018AnInteractive}. 

In the original D-COACH formulation, a demonstration session is required, at the beginning of the training process, for tuning the convolutional layers used for state dimensionality reduction. After that, a fully connected network policy (connected to the previously trained encoder) is interactively trained during task execution with the human corrective feedback, similarly to the human-agent interaction of the original COACH.

In this paper we introduce an enhanced version of D-COACH, which eliminates the need of demonstration sessions and trains the whole CNN simultaneously, reducing the time and effort of the user/coach for teaching a policy.  In D-COACH no reward functions are needed, and the amount of learning episodes are significantly reduced in comparison to alternative DRL approaches. Enhanced D-COACH is validated in three different problems through simulations and real-world scenarios. In each problem, %two % 
the original and enhanced D-COACH are analyzed and compared with the DDPG method. 


\section{Objectives}
\subsection{General Objective}
The general objective of this work is to develop and validate an algorithm that exploits the advantages of combining deep learning with human corrective feedback for learning policies when solving sequential decision-making problems in robotics. 

\subsection{Specific Objectives}

\begin{itemize}
    \item Study the state-of-the-art of robot learning algorithms for solving sequential decision-making problems.
    \item Propose and implement an algorithm that, by using deep neural networks policies, allows robots to learn to solve tasks in continuous-action domains from low-dimensional and high-dimensional states using online human corrective feedback. 
    \item Validate the proposed algorithm with tasks using simulated and real platforms.
    \item Propose a variation of the algorithm to solve partially observed problems.
\end{itemize}

\section{Hypothesis}
Human corrective feedback can be used to learn well performing deep neural network policies. As a consequence, an algorithm of this kind will effectively inherit the learning properties of using human corrective feedback and the function approximation properties of deep neural networks. Combining both approaches will result in a time-efficient and reward function free learning algorithm capable of solving problems using complex observations, such as raw pixels from an image.

\section{Outline}
This thesis is divided in four chapters. Chapters 2-4 introduce and evaluate three different variations of D-COACH:

\begin{itemize}
    \item \textbf{Chapter 1} presents the theoretical framework of this work. Both the areas of sequential decision-making and deep learning are revised in order to use them as baselines for the development of the thesis.
    \item \textbf{Chapter 2} introduces the first variation of D-COACH proposed in this work. The ideas taken from the COACH framework are discussed and the formulation of D-COACH is presented. This approach is validated using simulated and real-world platforms.
    \item \textbf{Chapter 3} proposes an enhanced variation of the D-COACH introduced in Chapter 2, tackling one of its main shortcomings. This new version of the algorithm is able to learn policies from scratch in problems with high-dimensional state spaces in one learning step, while the previous version of D-COACH had an extra offline learning step.
    \item \textbf{Chapter 4} introduces a variation of D-COACH that adds memory to the agents in order for them to solve tasks in partially observable environments. In contrast, Chapters 2 and 3 assume that the environments are fully observable, which is not true in many real-world scenarios.
\end{itemize}

Finally, the conclusions and the future work of this thesis are discussed. 

\end{intro}