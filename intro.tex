\chapter{Introduction}
\section{Motivation and Problem Statement}
In  recent years, outstanding results in complex decision-making problems have been obtained with Deep Reinforcement Learning (DRL). State-of-the-art algorithms have solved problems with large state-action spaces, such as playing Atari games \cite{atari}, beating the world champion in GO \cite{Silver2016} or defeating a top professional player in StarCraft II \cite{alphastarblog}, along with low level simulated continuous control tasks in environments such as the ones included in the OpenAI Gym \cite{brockman2016openai} and the DeepMind Control Suite \cite{tassa2018deepmind}. 

Nevertheless, in robotic problems, DRL is still limited in applications with real-world problems \cite{Gu2017}. Most of the tasks that have been successfully addressed with DRL have two common characteristics: 1) they have well-specified reward functions, and 2) they require large amounts of trials, which means long training periods (or powerful computers) to obtain a satisfying behavior. These two characteristics can be problematic in cases where 1) the goals of the tasks are poorly defined or hard to specify/model (reward function does not exist), 2) the execution of many trials is not feasible (real systems case) and/or not much computational power or time is available, and 3) sometimes additional external perception is necessary for computing the reward/cost function.

In this regard, the transfer of knowledge learned in a simulator to the real-world is a typical solution. However, the mismatch between the virtual and real environment, known as ``Reality Gap``, is often problematic \cite{koos2013transferability}. This results in agents that do not perform at their best in the real-world. Thus, it would be preferable to learn/fine-tune policies directly in the real-world.

In contrast, Machine Learning methods that rely on transfer of human knowledge, Interactive Machine Learning (IML) methods, have shown to be time efficient for obtaining good performance policies and may not require a well-specified reward function; moreover, some methods do not need expert human teachers for training high performance agents \cite{akrour2011preference,Knox:2009:ISA:1597735.1597738,Celemin2018AnInteractive}. In previous years, IML techniques were limited to work with low-dimensional state spaces problems and to the use of function approximation such as linear models of basis functions (choosing a right basis function set was crucial for successful learning), in the same way as Reinforcement Learning (RL). But, as DRL have showed, by approximating policies with Deep Neural Networks (DNNs) it is possible to solve problems with high-dimensional state spaces, without the need of feature engineering for preprocessing the states. If the same approach is used in IML, the DRL shortcomings mentioned before can be addressed with the support of human users who participate in the learning process of the agent.

Therefore, in this work we study the use of human corrective feedback during task execution to learn DNN policies in state spaces of low and high dimensionality in continuous action problems (which is the case for most of the problems in robotics).

In this context, this thesis proposes an alternative Interactive Machine Learning strategy for training Deep Neural Network policies based on human corrective feedback, with a method called Deep COACH (D-COACH). Deep Learning is combined with the COrrective Advice Communicated by Humans (COACH) framework, in which non-expert humans shape policies by correcting the agentâ€™s actions during execution. The D-COACH framework has the potential to solve complex problems without much data or time required. Experimental results validated the efficiency of the proposed framework in simulated and real-world platforms, with state spaces of low and high dimensionality, showing the capacity to successfully learn policies for continuous action spaces. 

Three variations of D-COACH are introduced:

\begin{enumerate}
    \item \textbf{Offline state representation learning (D-COACH)}
    \item \textbf{ONline state representation learning (D-COACH ON)}
    \item \textbf{Memoryful ONline state representation learning (D-COACH MON)}
\end{enumerate}

D-COACH and D-COACH ON assume that the agents interact in fully-observable environments. As a consequence, they make decisions solely based on what they are observing on that precise moment. In low-dimensional state problems, they do not differ. A fully connected network policy is interactively trained during task execution with human corrective feedback. Nevertheless, in high-dimensional state problems D-COACH ON enhances the methodology proposed in D-COACH. In both approaches a convolutional autoencoder is added to the network architecture for state dimensionality reduction. In the D-COACH formulation, a demonstration session is required at the beginning of the training process for tuning the weights of the autoencoder. After that, a fully connected network policy is linked to the output of the encoder layers (latent space of the autoencoder), and is trained interactively, in the same way as in the low-dimensional state case. In contrast, D-COACH ON eliminates the need of demonstration sessions and trains the policy and the autoencoder simultaneously, sharing the convolutional layers (encoder) between both networks. This reduces the time and effort of the human for teaching a policy.

D-COACH MON assumes that the agents interact in partially-observable environments, which is true in many real-world scenarios, and tries to cover a subset of these problems. Memory is added to the agents by using recurrent layers in the network architectures in order to obtain well performing policies in problems where past observations contain crucial information about the current state of the agent. As in D-COACH ON, everything is learned online, without the need of a demonstration session. 

Throughout this thesis, we give special emphasis to studying the use of policies parameterized with Convolutional Neural Networks (CNNs) for solving problems with high-dimensional state spaces, such as raw pixels from an image. CNNs give agents the possibility to build rich state representations of the environment without feature engineering on the side of the designer (which was always necessary in classical RL). These properties can be very useful in robotics, since it is common to find applications with high-dimensional observations, such as RGB images. Giving robots the ability to learn from such high-dimensional data will allow to scale up the complexity of what robots are able to do. All of the proposed variations of D-COACH have as common denominator the study of CNNs,  seeking to exploit their properties in the best possible way. 

\section{Objectives}
\subsection{General Objective}
The general objective of this thesis is to develop and validate an algorithm that exploits the advantages of combining deep learning with human corrective feedback for learning policies when solving sequential decision-making problems in robotics. 

\subsection{Specific Objectives}

\begin{itemize}
    \item Study the state-of-the-art of robot learning algorithms for solving sequential decision-making problems.
    \item Propose and implement an algorithm that, by using deep neural networks policies, allows robots to learn to solve tasks in continuous-action domains from low-dimensional and high-dimensional states using online human corrective feedback. 
    \item Validate the proposed algorithm with tasks using simulated and real platforms.
    \item Propose a variation of the algorithm to solve problems with partially observed environments.
\end{itemize}

\section{Hypothesis}
Human corrective feedback can be used to learn well performing deep neural network policies. As a consequence, an algorithm of this kind will effectively inherit the learning properties of using human corrective feedback and the function approximation properties of deep neural networks. Combining both approaches will result in a time-efficient and reward function free learning algorithm capable of solving problems using complex observations, such as raw pixels from an image.

\section{Outline}
This thesis is divided in four chapters. Chapters 2-4 introduce and evaluate three different variations of D-COACH:

\begin{itemize}
    \item \textbf{Chapter 1} presents the theoretical framework of this work. Both the areas of sequential decision-making and deep learning are revised in order to use them as baselines for the development of the thesis.
    \item \textbf{Chapter 2} introduces D-COACH. The ideas taken from the COACH framework are discussed and the formulation of D-COACH is presented. This approach is validated using simulated and real-world platforms.
    \item \textbf{Chapter 3} proposes an enhanced variation of D-COACH introduced in Chapter 2 (D-COACH On), tackling one of its main shortcomings. This new version of the algorithm is able to learn policies from scratch in problems with high-dimensional state spaces in one learning step, while the previous version of D-COACH had an extra offline learning step.
    \item \textbf{Chapter 4} introduces a variation of D-COACH that adds memory to the agents (D-COACH v3) in order for them to solve tasks in partially observable environments. In contrast, Chapters 2 and 3 assume that the environments are fully observable, which is not true in many real-world scenarios.
\end{itemize}

Finally, the conclusions and the future work of this thesis are discussed.