\documentclass[upright, contnum]{umemoria}
\depto{Departamento de Ingeniería Eléctrica}
\author{Rodrigo Javier Pérez Dattari}
\title{Interactive Learning with Corrective Feedback for Continuous-Action Policies based on Deep Neural Networks}
\auspicio{FONDECYT Projecto 1161500.}
\date{2018}
\guia{Javier Ruiz del Solar}
\carrera{Magíster en Ciencias de la Ingeniería, Mención Eléctrica}
\carreraeng{Master of Engineering Sciences in Electrical Engineering}
\memoria{Tesis para optar al Grado de \break  Magíster en Ciencias de la Ingeniería, Mención Eléctrica\break \break
Memoria para optar al Título de Ingeniero Civil Eléctrico}
\comision{Carlos Celemin}

\usepackage{lipsum}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{titlesec}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{blindtext, graphicx}
\newcommand{\note}[1]{\color{red}\textbf{{#1}}\color{black}}

\titlespacing*{\section}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\subsection}{0pt}{\baselineskip}{\baselineskip}
\titlespacing*{\subsubsection}{0pt}{\baselineskip}{\baselineskip}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\frontmatter
\maketitle

\begin{abstract}

\end{abstract}

\begin{abstract_eng}
Deep Reinforcement Learning (DRL) has become a powerful methodology to solve complex decision making problems. However, DRL has several limitations when used in real-world problems (e.g., robotics applications). For instance, long training times are required and cannot be accelerated in contrast to simulated environments, and reward functions may be hard to specify/model and/or to compute. Moreover, the transfer of policies learned in a simulator to the real-world has limitations (reality gap). On the other hand, machine learning methods that rely on the transfer of human knowledge to an agent have shown to be time efficient for obtaining well performing policies and do not require a reward function. In this context, we analyze the use of human corrective feedback during task execution to learn policies with high-dimensional state spaces, by using the D-COACH framework, and we propose new variants of this framework. D-COACH is a Deep Learning based extension of COACH (COrrective Advice Communicated by Humans), where humans are able to shape policies through corrective advice. The enhanced version of D-COACH, which is proposed in this paper, largely reduces the time and effort of a human for training a policy. Experimental results validate the efficiency of the D-COACH framework in three different problems (simulated and with real robots), and show that its enhanced version reduces the human training effort considerably, and makes it feasible to learn policies within periods of time in which a DRL agent do not reach any improvement.
\end{abstract_eng}


\begin{dedicatoria}
A mi abuelo Ernesto.
\end{dedicatoria}

\begin{thanks}

\end{thanks}

\cleardoublepage
\tableofcontents
\cleardoublepage
\listoftables
\cleardoublepage
\listoffigures

\mainmatter

\input{intro.tex}
\input{cap1.tex}
\input{cap2.tex}
\input{cap3.tex}
\input{cap4.tex}
\input{conclu.tex}

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibliography}
\end{document}
