% This file was created with JabRef 2.10.
% Encoding: UTF-8

@article{Warnell2017,
archivePrefix = {arXiv},
arxivId = {1709.10163},
author = {Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
eprint = {1709.10163},
file = {::},
month = {sep},
title = {{Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}},
url = {http://arxiv.org/abs/1709.10163},
year = {2017}
}

@article{Silver2016,
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
title={Mastering the game of Go with deep neural networks and tree search},
journal={nature},
volume={529},
number={7587},
pages={484},
year={2016},
publisher={Nature Publishing Group}
}
@misc{atari,
abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
added-at = {2014-12-14T17:55:47.000+0100},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
biburl = {https://www.bibsonomy.org/bibtex/2f4760edc252cd402821a341bda0026bf/vch},
description = {Playing Atari with Deep Reinforcement Learning},
interhash = {78966703f649bae69a08a6a23a4e8879},
intrahash = {f4760edc252cd402821a341bda0026bf},
keywords = {arxiv cs},
note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
timestamp = {2014-12-14T17:55:47.000+0100},
title = {Playing Atari with Deep Reinforcement Learning},
url = {http://arxiv.org/abs/1312.5602},
year = 2013
}
@inproceedings{Gu2017,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA), 2017 },
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}
@inproceedings{Christiano2017,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4299--4307},
  year={2017}
}
@inproceedings{Celemin2015,
abstract = {COACH (COrrective Advice Communicated by Humans), a new interactive learning framework that allows non-expert humans to shape a policy through corrective advice, using a binary signal in the action domain of the agent, is proposed. One of the main innovative features of COACH is a mechanism for adaptively adjusting the amount of human feedback that a given action receives, taking into consideration past feedback. The performance of COACH is compared with the one of TAMER (Teaching an Agent Manually via Evaluative Reinforcement), ACTAMER (Actor-Critic TAMER), and an autonomous agent trained using SARSA(?) in two reinforcement learning problems. COACH outperforms all other learning frameworks in the reported experiments. In addition, results show that COACH is able to transfer successfully human knowledge to agents with continuous actions, being a complementary approach to TAMER, which is appropriate for teaching in discrete action domains.},
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-29339-4_2},
isbn = {9783319293387},
issn = {16113349},
keywords = {Ball dribbling,Human feedback in action domains,Human teachers,Interactive learning,Robot learning,Robot soccer},
title = {{Interactive learning of continuous actions from corrective advice communicated by humans}},
year = {2015}
}
@inproceedings{Celemin2016,
abstract = {{\textcopyright} 2016 IEEE. COACH (COrrective Advice Communicated by Humans) is an interactive learning framework that allows non-expert humans to shape a policy through corrective advice, using a binary signal in the action domain of the agent. The original COACH formulation has been tested in problems of one-dimensional actions spaces with RBF linear models for the policy approximation. In this paper the COACH framework is tested with two more complex learning problems of more than one dimension in the action domain such as learning to drive a bicycle, and ball-dribbling with humanoid robots. Moreover, for the second problem, the COACH's principles are extended for training a decision-making system using a fuzzy based policy approximation. In these two problems the performance of COACH is compared with the one of other learning methods, obtaining better results. Results show that COACH is able to transfer successfully human knowledge to agents with multi-dimensional continuous action domains based on the use of different kind of models.},
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
booktitle = {2016 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2016 - Proceedings},
doi = {10.1109/LA-CCI.2016.7885734},
isbn = {9781509051052},
keywords = {Learning from Demonstration,Robot learning,ball dribbling,human feedback in action domains,human teachers,interactive learning,robot soccer},
title = {{Teaching agents with corrective human feedback for challenging problems}},
year = {2016}
}
@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple com-putational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previ-ous " parameter server " designs the management of shared state is built into the system, TensorFlow enables develop-ers to experiment with novel optimizations and training al-gorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural net-works. Several Google services use TensorFlow in pro-duction, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Brain, Google},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16)},
eprint = {1605.08695},
isbn = {978-1-931971-33-1},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogenous Distributed Systems}},
year = {2016}
}

@article{Argall2009,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@book{Chernova2014,
  title={Robot learning from human teachers},
  author={Chernova, Sonia and Thomaz, Andrea L},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={8},
  number={3},
  pages={1--121},
  year={2014},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{Knox:2009:ISA:1597735.1597738,
 author = {Knox, W. Bradley and Stone, Peter},
 title = {Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
 booktitle = {Proceedings of the Fifth International Conference on Knowledge Capture},
 series = {K-CAP '09},
 year = {2009},
 isbn = {978-1-60558-658-8},
 location = {Redondo Beach, California, USA},
 pages = {9--16},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1597735.1597738},
 doi = {10.1145/1597735.1597738},
 acmid = {1597738},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {human teachers, human-agent interaction, learning agents, sequential decision-making, shaping},
} 

@article{Mnih2015,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{Bellman1957,
abstract = {IFanyt hing, dynamic programming, at its adventin the 1950s, had a greater impact on the engineering community than did the maximum principle (and a lesser impact on the mathemat-ical community). While the maximum principle emerged from its womb in the calculus of variations as a fully formulated necessary condition of optimality for open-loop optimal con-trol problems, dynamic programming emerged theoretically re-stricted, requiring strong smoothness assumptions not often sat-isfied. Nonetheless dynamic programming stimulated research enormously providing, uniquely, three things. Firstly, it gave the research community a powerful conceptual framework for for-mulating feedback control and decision problems, deterministic and stochastic; it remains today the only general methodology for handling optimal feedback control problems when stochas-tic disturbances are present [11]. Bellman's book [1] and the textbook [2] give some impression of the wide range of feed-back problems addressed: multi-stage allocation and decision processes, inventory and stock control, production processes, control of queues, Markovian decision problems, combinatorial problems as well as the problems, more familiar to us, of control-ling deterministic and stochastic systems described by difference or differential equations. Secondly, it provided a constructive method for solving these problems via the recurrence relation},
author = {Bellman, Richard},
doi = {10.1109/9780470544334.ch6},
journal = {Dynamic Programming},
title = {{The Structure of Dynamic Programming Processes}},
year = {1957}
}

@article{zhang2017deep,
  title={Deep imitation learning for complex manipulation tasks from virtual reality teleoperation},
  author={Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Goldberg, Ken and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.04615},
  year={2017}
}

@article{Argall2011,
abstract = {Task demonstration is an effective technique for developing robot motion control policies. As tasks become more complex, however, demonstration can become more difficult. In this work, we introduce an algorithm that uses corrective human feedback to build a policy able to perform a novel task, by combining simpler policies learned from demonstration. While some demonstration-based learning approaches do adapt policies with execution experience, few provide corrections within low-level motion control domains or to enable the linking of multiple of demonstrated policies. Here we introduce Feedback for Policy Scaffolding (FPS) as an algorithm that first evaluates and corrects the execution of motion primitive policies learned from demonstration. The algorithm next corrects and enables the execution of a more complex task constructed from these primitives. Key advantages of building a policy from demonstrated primitives is the potential for primitive policy reuse within multiple complex policies and the faster development of these policies, in addition to the development of complex policies for which full demonstration is difficult. Policy reuse under our algorithm is assisted by human teacher feedback, which also contributes to the improvement of policy performance. Within a simulated robot motion control domain we validate that, using FPS, a policy for a novel task is successfully built from motion primitives learned from demonstration. We show feedback to both aid and enable policy development, improving policy performance in success, speed and efficiency. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Browning, Brett and Veloso, Manuela M.},
doi = {10.1016/j.robot.2010.11.004},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Demonstration learning,Policy reuse,Robot motion control,Teacher feedback},
title = {{Teacher feedback to scaffold and refine demonstrated motion primitives on a mobile robot}},
year = {2011}
}

@inproceedings{Argall2008,
  title={Learning robot motion control with demonstration and advice-operators},
  author={Argall, Brenna D and Browning, Brett and Veloso, Manuela},
  booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={399--404},
  year={2008},
  organization={IEEE}
}


@inproceedings{thomaz2006reinforcement,
  title={Reinforcement learning with human teachers: Understanding how people want to teach robots},
  author={Thomaz, Andrea L and Hoffman, Guy and Breazeal, Cynthia},
  booktitle={Robot and Human Interactive Communication, 2006. ROMAN 2006. The 15th IEEE International Symposium on},
  pages={352--357},
  year={2006},
  organization={IEEE}
}

@inproceedings{akrour2011preference,
  title={Preference-based policy learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={12--27},
  year={2011},
  organization={Springer}
}

@article{suay2012practical,
  title={A practical comparison of three robot learning from demonstration algorithm},
  author={Suay, Halit Bener and Toris, Russell and Chernova, Sonia},
  journal={International Journal of Social Robotics},
  volume={4},
  number={4},
  pages={319--330},
  year={2012},
  publisher={Springer}
}

@inproceedings{celeminhuman,
  title={Human Corrective Advice in the Policy Search Loop},
  author={Celemin, Carlos and Maeda, Guilherme and Kober, Jens and Ruiz-del-Solar, Javier},
  booktitle={Workshop Human-in-the-loop Robotic Manipulation: On the Influence of the Human Role, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2017},
  organization={IEEE}
}

@article{Celemin2018AnInteractive,
  title={An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
  author={Celemin, Carlos and Ruiz-del-Solar, Javier},
  journal={Journal of Intelligent \& Robotic Systems},
  pages={1--21},
  year={2018},
  publisher={Springer},
  organization={Springer}
}

@article{Lillicrap2015,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{Paull2017,
abstract = {— Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (" Duckiebots ") built from off-the-shelf components, and cities (" Duckietowns ") complete with roads, signage, traffic lights, obstacles, and citizens (duck-ies) in need of transportation. The advantage of the Duckietown platform is the combination of its low cost with its large range of functionalities. A Duckiebot senses the world with only one monocular camera and performs all processing onboard with a Raspberri Pi 2, yet it is able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots; localize within a global map; navigate a city; and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool for education, since institutions can save money and time by not having to develop all of the supporting infrastructure and capabilities that are tangential to the desired course material. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research. Four other institutions have used Duckietown to date.},
author = {Paull, Liam and Tani, Jacopo and Ahn, Heejin and Alonso-Mora, Javier and Carlone, Luca and Cap, Michal and Chen, Yu Fan and Choi, Changhyun and Dusek, Jeff and Fang, Yajun and Hoehener, Daniel and Liu, Shih Yuan and Novitzky, Michael and Okuyama, Igor Franzoni and Pazis, Jason and Rosman, Guy and Varricchio, Valerio and Wang, Hsueh Cheng and Yershov, Dmitry and Zhao, Hang and Benjamin, Michael and Carr, Christopher and Zuber, Maria and Karaman, Sertac and Frazzoli, Emilio and {Del Vecchio}, Domitilla and Rus, Daniela and How, Jonathan and Leonard, John and Censi, Andrea},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989179},
isbn = {9781509046331},
issn = {10504729},
pages = {1497--1504},
title = {{Duckietown: An open, inexpensive and flexible platform for autonomy education and research}},
year = {2017}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}


@article{koos2013transferability,
  title={The transferability approach: Crossing the reality gap in evolutionary robotics},
  author={Koos, Sylvain and Mouret, Jean-Baptiste and Doncieux, St{\'e}phane},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={17},
  number={1},
  pages={122--145},
  year={2013},
  publisher={IEEE}
}


@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}


@article{tassa2018deepmind,
  title={DeepMind Control Suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@misc{gym_duckietown,
  author = {Maxime Chevalier-Boisvert, Florian Golemo, Yanjun Cao, Liam Paull},
  title = {Duckietown Environments for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/duckietown/gym-duckietown}},
}

@inproceedings{Finn2015,
  title={Deep spatial autoencoders for visuomotor learning},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={512--519},
  year={2016},
  organization={IEEE}
}

@article{Ha2018,
  title={World Models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}


@inproceedings{perez2018interactive,
  title={Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks},
  author={Perez, Rodrigo and Celemin, Carlos and Ruiz-del-Solar, Javier and Kober, Jens},
  booktitle={International Symposium on Experimental Robotics},
  year={2018},
  organization={Springer}
}

@article{springenberg2014striving,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1412.6806},
  year={2014}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}

@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{franccois2018introduction,
  title={An Introduction to Deep Reinforcement Learning},
  author={Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={3-4},
  pages={219--354},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@inproceedings{fails2003interactive,
  title={Interactive machine learning},
  author={Fails, Jerry Alan and Olsen Jr, Dan R},
  booktitle={Proceedings of the 8th international conference on Intelligent user interfaces},
  pages={39--45},
  year={2003},
  organization={ACM}
}

@article{ware2001interactive,
  title={Interactive machine learning: letting users build classifiers},
  author={Ware, Malcolm and Frank, Eibe and Holmes, Geoffrey and Hall, Mark and Witten, Ian H},
  journal={International Journal of Human-Computer Studies},
  volume={55},
  number={3},
  pages={281--292},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{amershi2012regroup,
  title={Regroup: Interactive machine learning for on-demand group creation in social networks},
  author={Amershi, Saleema and Fogarty, James and Weld, Daniel},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={21--30},
  year={2012},
  organization={ACM}
}

@article{ngo2014efficient,
  title={Efficient interactive multiclass learning from binary feedback},
  author={Ngo, Hung and Luciw, Matthew and Nagi, Jawas and Forster, Alexander and Schmidhuber, J{\"u}rgen and Vien, Ngo Anh},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={4},
  number={3},
  pages={12},
  year={2014},
  publisher={ACM}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2285--2294},
  year={2017},
  organization={JMLR. org}
}

@article{chernova2009interactive,
  title={Interactive policy learning through confidence-based autonomy},
  author={Chernova, Sonia and Veloso, Manuela},
  journal={Journal of Artificial Intelligence Research},
  volume={34},
  pages={1--25},
  year={2009}
}

@inproceedings{mericcli2010complementary,
  title={Complementary humanoid behavior shaping using corrective demonstration},
  author={Meri{\c{c}}li, Cetin and Veloso, Manuela and Akin, H Levent},
  booktitle={2010 10th IEEE-RAS International Conference on Humanoid Robots},
  pages={334--339},
  year={2010},
  organization={IEEE}
}

@article{argall2009learning,
  title={Learning mobile robot motion control from demonstration and corrective feedback},
  author={Argall, Brenna D},
  journal={Diss. University of Southern California},
  year={2009}
}

@inproceedings{argall2008learning,
  title={Learning robot motion control with demonstration and advice-operators},
  author={Argall, Brenna D and Browning, Brett and Veloso, Manuela},
  booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={399--404},
  year={2008},
  organization={IEEE}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@techreport{lin1993reinforcement,
  title={Reinforcement learning for robots using neural networks},
  author={Lin, Long-Ji},
  year={1993},
  institution={CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE}
}

@article{zhang2017deeper,
  title={A deeper look at experience replay},
  author={Zhang, Shangtong and Sutton, Richard S},
  journal={arXiv preprint arXiv:1712.01275},
  year={2017}
}

@book{busoniu2010reinforcement,
  title={Reinforcement learning and dynamic programming using function approximators},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart and Ernst, Damien},
  year={2010},
  publisher={CRC press}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@book{nielsen2015neural,
  title={Neural networks and deep learning},
  author={Nielsen, Michael A},
  volume={25},
  year={2015},
  publisher={Determination press USA}
}

@article{saad1998online,
  title={Online algorithms and stochastic approximations},
  author={Saad, David},
  journal={Online Learning},
  volume={5},
  year={1998},
  publisher={Cambridge Univ. Press}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{amershi2014power,
  title={Power to the people: The role of humans in interactive machine learning},
  author={Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  journal={AI Magazine},
  volume={35},
  number={4},
  pages={105--120},
  year={2014}
}

@article{billing2010formalism,
  title={A formalism for learning from demonstration},
  author={Billing, Erik A and Hellstr{\"o}m, Thomas},
  journal={Paladyn},
  volume={1},
  number={1},
  pages={1--13},
  year={2010},
  publisher={Springer}
}

@article{chernova2014robot,
  title={Robot learning from human teachers},
  author={Chernova, Sonia and Thomaz, Andrea L},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={8},
  number={3},
  pages={1--121},
  year={2014},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{cuayahuitl2013machine,
  title={Machine learning for interactive systems and robots: a brief introduction},
  author={Cuay{\'a}huitl, Heriberto and van Otterlo, Martijn and Dethlefs, Nina and Frommberger, Lutz},
  booktitle={Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication},
  pages={19--28},
  year={2013},
  organization={ACM}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{bellman1957dynamic,
  title={Dynamic Programming},
  author={Bellman, Richard Ernest},
  year={1957},
  publisher={Courier Dover Publications}
}

@article{de2018integrating,
  title={Integrating state representation learning into deep reinforcement learning},
  author={de Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu{\v{s}}ka, Robert},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={3},
  pages={1394--1401},
  year={2018},
  publisher={IEEE}
}

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@inproceedings{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={2015 AAAI Fall Symposium Series},
  year={2015}
}

@inproceedings{lample2017playing,
  title={Playing FPS games with deep reinforcement learning},
  author={Lample, Guillaume and Chaplot, Devendra Singh},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@article{celemin2018fast,
  title={A fast hybrid reinforcement learning framework with human corrective feedback},
  author={Celemin, Carlos and Ruiz-del-Solar, Javier and Kober, Jens},
  journal={Autonomous Robots},
  pages={1--14},
  year={2018},
  publisher={Springer}
}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojciech M. and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Wu, Yuhuai and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}